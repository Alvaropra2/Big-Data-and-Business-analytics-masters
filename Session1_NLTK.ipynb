{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fabcea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First steps in NLP\n",
    "# Practicing NLTK \n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3229de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Text: I would love to have pizza for dinner\n"
     ]
    }
   ],
   "source": [
    "# 1. Creating a sentence that is going to be the entry of our chain NLP\n",
    "\n",
    "text = \"I would love to have pizza for dinner\"\n",
    "print(\"\\n\\n1. Text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff1faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. Phrases: ['I would love to have pizza for dinner']\n"
     ]
    }
   ],
   "source": [
    "# 2. Lets divide the text into phrases\n",
    "\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "print(\"\\n\\n2. Phrases:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697f5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3. Tokens: ['I', 'would', 'love', 'to', 'have', 'pizza', 'for', 'dinner']\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenization of the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"\\n\\n3. Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf12de36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4. Morphological analysis: [('I', 'PRP'), ('would', 'MD'), ('love', 'VB'), ('to', 'TO'), ('have', 'VB'), ('pizza', 'NN'), ('for', 'IN'), ('dinner', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Morphological analysis\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(\"\\n\\n4. Morphological analysis:\", tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a470c7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5. Stems:\n",
      "i\n",
      "would\n",
      "love\n",
      "to\n",
      "have\n",
      "pizza\n",
      "for\n",
      "dinner\n"
     ]
    }
   ],
   "source": [
    "#Stemming: extracting the root \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(\"\\n\\n5. Stems:\")\n",
    "for tok in tokens:\n",
    "    print(stemmer.stem(tok.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16c8d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "6. Lemas: \n",
      "i\n",
      "would\n",
      "love\n",
      "to\n",
      "have\n",
      "pizza\n",
      "for\n",
      "dinner\n"
     ]
    }
   ],
   "source": [
    "# Lematizacion\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#El lematizador de wordnet solo reconoce 4 etiquetas POS: a (adjetivo), r(adverbio),n (nombre),v(verbo). \n",
    "#Así que debemos hacer una conversión del formato Penn Tree Bank al formato wordnet (ej: NN->n, JJ->a, RB->r, VB->V, ...)\n",
    "from nltk.corpus import wordnet\n",
    "wnTags = {'N':wordnet.NOUN,'J':wordnet.ADJ,'V':wordnet.VERB,'R':wordnet.ADV} \n",
    "print (\"\\n\\n\\n6. Lemas: \")\n",
    "for (tok,tag) in tagged:\n",
    "    #wordnet no contiene las formas abreviadas 'm , 's y  n't así que las introducimos nosotros para que lematice bien\n",
    "    if tok=='\\'m':\n",
    "        tok = 'am'\n",
    "    if tok=='\\'s':\n",
    "        tok = 'is'\n",
    "    if tok=='n\\'t':\n",
    "        tok = 'not'\n",
    "    tag = tag[:1]\n",
    "    lemma = lemmatizer.lemmatize(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "    if lemma is None: #Si wordnet no contiene la palabra, supondremos que el lema es igual al token\n",
    "       lemma = tok.lower() \n",
    "    print (lemma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2795845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisis sintactico:\n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pijamas))))) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "groucho = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pijamas'] #\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pijamas'\n",
    "V -> 'shot' | 'did'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "#Generamos un parser sintáctico capaz de reconocer la gramática\n",
    "parser = nltk.ChartParser(grammar)\n",
    "print ('Analisis sintactico:\\n')\n",
    "for tree in parser.parse(groucho):\n",
    "    print(tree,'\\n')\n",
    "    tree.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
