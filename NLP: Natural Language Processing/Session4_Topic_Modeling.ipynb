{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f691f2",
   "metadata": {},
   "source": [
    "## Text Mining: Topic Modeling\n",
    "To apply clustering using K-Means with tfidf vectorizer, we are going to use the example into that URL. That example use a vectorizer to getting the tfidf of all words in a document. This vectorizer is TfidfVectorizer.\n",
    "\n",
    "We are going to use the 20newgroups corpus and select two group: alt.atheis and sci.space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51e1ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries \n",
    "import nltk \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a03c7",
   "metadata": {},
   "source": [
    "#### Load the corpus of texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd639b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories: ['alt.atheism', 'sci.space']\n",
      "['alt.atheism', 'sci.space']\n",
      "\n",
      "1786 documents\n"
     ]
    }
   ],
   "source": [
    "# Load some categories from the training test \n",
    "categories = ['alt.atheism','sci.space']\n",
    "print(\"Loading 20 newsgroups dataset for categories:\", categories)\n",
    "\n",
    "print(categories)\n",
    "print()\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "trainCorpus = dataset.data\n",
    "print(\"%d documents\" % len(trainCorpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb9416",
   "metadata": {},
   "source": [
    "#### Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40b01167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tf Matrix:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "number of sentences 1786, number of features 28382\n"
     ]
    }
   ],
   "source": [
    "# Create the vectorizer \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the vectorizer to transform the documents on a matrix of tf's (term frequency) of documents\n",
    "vectorizer.fit(trainCorpus)\n",
    "\n",
    "# Extract the terms frequency\n",
    "tfMatrix = vectorizer.transform(trainCorpus)\n",
    "\n",
    "# Print the matrix. \n",
    "# This matrix converted in array indicates: \n",
    "#    - Each column is a one feature, \n",
    "#    - Each row is a one sentence of corpus.\n",
    "#    - The (i,j) value indicates the frequency of j feature in a i sentence\n",
    "print('\\ntf Matrix:\\n', tfMatrix.toarray())\n",
    "\n",
    "# Print the shape of our matrix:\n",
    "print(\"number of sentences %d, number of features %d\" % tfMatrix.shape)\n",
    "\n",
    "# Get all features \n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613a09e",
   "metadata": {},
   "source": [
    "#### Apply LDA Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d79221cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 28382)\n",
      "(1786, 10)\n"
     ]
    }
   ],
   "source": [
    "topics = 10\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components =topics, max_iter=5, \n",
    "                                      learning_method='online', \n",
    "                                      learning_offset=50.,\n",
    "                                      random_state=0).fit(tfMatrix)\n",
    "\n",
    "# Variational parameters for topic word distribution. \n",
    "#  Since the complete conditional for topic word distribution is a Dirichlet, \n",
    "#  components_[i, j] can be viewed as pseudocount that represents the number \n",
    "#  of times word j was assigned to topic i. It can also be viewed as distribution \n",
    "#  over the words for each topic after normalization:\n",
    "topic_word_distribution = lda_model.components_\n",
    "\n",
    "# Document topic distribution for tfMatrix.\n",
    "document_topic_distribution = lda_model.transform(tfMatrix)\n",
    "\n",
    "print(topic_word_distribution.shape)\n",
    "print(document_topic_distribution.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a23aa3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display topics depart from:\n",
    "#  - H: Variational parameters for topic word distribution.\n",
    "#  - W: Document topic distribution.\n",
    "#  - feature_names: Names of each corpus feature\n",
    "#  - documents: Corpus\n",
    "#  - no_top_words: Maximum number of words (topic) to display\n",
    "#  - no_top_documents: Maximum number of corpus document to display\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"\\nTopic %d:\" % (topic_idx))\n",
    "        for i in topic.argsort()[:-no_top_words - 1:-1]:\n",
    "            print(\" \",feature_names[i])\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(documents[doc_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76ed1881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "  af\n",
      "  afit\n",
      "  mil\n",
      "  elements\n",
      "\n",
      "Topic 1:\n",
      "  het\n",
      "  een\n",
      "  van\n",
      "  te\n",
      "\n",
      "Topic 2:\n",
      "  kadie\n",
      "  of\n",
      "  the\n",
      "  pto\n",
      "\n",
      "Topic 3:\n",
      "  space\n",
      "  launch\n",
      "  satellite\n",
      "  pub\n",
      "\n",
      "Topic 4:\n",
      "  arms\n",
      "  the\n",
      "  permanet\n",
      "  prado\n",
      "\n",
      "Topic 5:\n",
      "  de\n",
      "  van\n",
      "  het\n",
      "  een\n",
      "\n",
      "Topic 6:\n",
      "  the\n",
      "  of\n",
      "  to\n",
      "  and\n",
      "\n",
      "Topic 7:\n",
      "  the\n",
      "  of\n",
      "  alcbel\n",
      "  devdjn\n",
      "\n",
      "Topic 8:\n",
      "  wam\n",
      "  the\n",
      "  comet\n",
      "  emr\n",
      "\n",
      "Topic 9:\n",
      "  the\n",
      "  globe\n",
      "  xpresso\n",
      "  of\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 4\n",
    "no_top_documents = 0\n",
    "\n",
    "display_topics(topic_word_distribution, document_topic_distribution, \n",
    "               tf_feature_names, trainCorpus, no_top_words, no_top_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
